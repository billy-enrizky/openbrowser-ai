---
title: "MCP Server Comparison"
description: "Benchmark comparison of OpenBrowser MCP vs Playwright MCP vs Chrome DevTools MCP"
icon: "scale-balanced"
---

# Browser MCP Server Comparison

Benchmark date: 2026-02-19 | OpenBrowser MCP v0.1.22 (CodeAgent, 1 tool) | Playwright MCP (latest) | Chrome DevTools MCP (latest)

## Overview

Three approaches to browser automation via MCP, measured on identical tasks.

| | Playwright MCP | Chrome DevTools MCP | OpenBrowser MCP |
|---|---|---|---|
| **Maintainer** | Microsoft | Chrome DevTools team (Google) | OpenBrowser |
| **GitHub Stars** | 27,300+ | 25,900+ | -- |
| **Engine** | Playwright (Chromium/Firefox/WebKit) | Puppeteer (CDP) | Raw CDP (direct) |
| **Tools** | 22 core (39 total) | 26 | 1 (`execute_code`) |
| **Approach** | A11y snapshot with every action | A11y snapshot on demand | CodeAgent -- Python code execution with browser namespace |
| **Element IDs** | `ref` from snapshot (`[ref=e25]`) | `uid` from snapshot (`uid=1_2`) | Numeric index from state |
| **Transport** | stdio | stdio | stdio |
| **Language** | TypeScript (Node.js) | TypeScript (Node.js) | Python |

## Why OpenBrowser Uses Fewer Tokens

OpenBrowser's CodeAgent architecture processes browser state **server-side** in a Python runtime. The LLM writes code that navigates, inspects DOM, and extracts data -- the full page content never enters the LLM context window. Only `print()` output comes back.

With Playwright and Chrome DevTools, the full page accessibility snapshot (124K-135K tokens for Wikipedia) is returned directly as tool output. The LLM reads the entire snapshot to find what it needs.

| Design Decision | Playwright MCP | Chrome DevTools MCP | OpenBrowser MCP |
|----------------|---------------|---------------------|-----------------|
| Navigate response | Full a11y snapshot | URL confirmation | URL confirmation |
| State query | Always full snapshot | Full a11y snapshot (`take_snapshot`) | Code processes state server-side |
| Click response | Updated snapshot | Action confirmation | Action confirmation |
| Text extraction | No dedicated tool | No dedicated tool | `evaluate()` + Python processing |
| Element identification | LLM reads snapshot | LLM reads snapshot | Code iterates selector_map server-side |

**Playwright MCP**: Every navigation returns the full page accessibility snapshot (~124K tokens for Wikipedia). The LLM sees everything immediately and can answer in 1-2 tool calls, but pays for processing the entire page.

**Chrome DevTools MCP**: Returns minimal confirmations for navigation. The agent must explicitly call `take_snapshot` to see the page (~135K tokens for Wikipedia). Snapshot size is comparable to Playwright's, but only sent on demand.

**OpenBrowser MCP**: The LLM writes Python code that processes browser state inside the server-side runtime. Only extracted results come back. The LLM relies on general web knowledge to write speculative code, and gets compact feedback (~30-800 chars per call) rather than full page dumps.

### What Each Server Returns (Verbatim)

All responses below are real output captured from each MCP server on the httpbin.org/forms/post page.

#### Navigate

**Playwright MCP** -- returns full a11y snapshot with every navigation (~2,150 chars on httpbin, ~496K chars on Wikipedia):

```yaml
### Ran Playwright code
await page.goto('https://httpbin.org/forms/post');
### Page
- Page URL: https://httpbin.org/forms/post
- Console: 1 errors, 0 warnings
### Snapshot
- generic [ref=e2]:
  - paragraph [ref=e3]:
    - generic [ref=e4]:
      - text: "Customer name:"
      - textbox "Customer name:" [ref=e5]
  - paragraph [ref=e6]:
    - generic [ref=e7]:
      - text: "Telephone:"
      - textbox "Telephone:" [ref=e8]
  ... (entire page tree continues)
```

**Chrome DevTools MCP** -- returns URL confirmation only (~136 chars):

```
# navigate_page response
Successfully navigated to https://httpbin.org/forms/post.
## Pages
1: https://httpbin.org/forms/post [selected]
```

**OpenBrowser MCP** -- returns URL confirmation only (~105 chars):

```
Navigated to: https://httpbin.org/forms/post
```

#### Get Page State / Snapshot

**Playwright MCP** `browser_snapshot` -- full a11y tree again (~1,896 chars on httpbin, ~495K chars on Wikipedia):

```yaml
- generic [ref=e2]:
  - paragraph [ref=e3]:
    - generic [ref=e4]:
      - text: "Customer name:"
      - textbox "Customer name:" [ref=e5]
  ... (entire page tree)
```

**Chrome DevTools MCP** `take_snapshot` -- full a11y tree (~1,214 chars on httpbin, ~538K chars on Wikipedia):

```
uid=1_0 RootWebArea url="https://httpbin.org/forms/post"
  uid=1_1 StaticText "Customer name: "
  uid=1_2 textbox "Customer name: "
  uid=1_3 StaticText "Telephone: "
  uid=1_4 textbox "Telephone: "
  uid=1_5 StaticText "E-mail address: "
  uid=1_6 textbox "E-mail address: "
  ... (entire page tree)
```

**OpenBrowser MCP** -- the LLM writes Python code to query exactly what it needs:

```python
# Get page metadata
state = await browser.get_browser_state_summary()
print(json.dumps({"url": state.url, "title": state.title}))
```

```json
{"url": "https://httpbin.org/forms/post", "title": "httpbin.org/forms/post"}
```

The agent requests only the data it needs via Python code -- page title, specific element attributes, or targeted JS evaluation. No full-page dumps.

#### Click / Type / Go Back

All three servers return short confirmations for actions:

| Operation | Playwright MCP | Chrome DevTools MCP | OpenBrowser MCP |
|-----------|---------------|---------------------|-----------------|
| Click | `Clicked element` | `Clicked element uid=1_2` | `Clicked element 5` |
| Type | Updated snapshot (~526 chars) | `Filled element uid=1_2` | `Typed 'John Doe' into element 4` |
| Go back | ~198 chars | ~149 chars | `Navigated back` |

#### Targeted Extraction (OpenBrowser only)

No equivalent in Playwright or Chrome DevTools MCP -- both require dumping the full snapshot or running JavaScript from the client side.

**Element search** -- the LLM writes Python to find specific elements:

```python
state = await browser.get_browser_state_summary()
for idx, el in state.dom_state.selector_map.items():
    if 'submit' in el.get_all_children_text(max_depth=1).lower():
        print(f"Found submit button at index {idx}")
```

**Data extraction** -- extract specific data via JS evaluation, not full-page dumps:

```python
# Extract only what's needed from a 97K-char Wikipedia page
data = await evaluate('document.querySelector(".infobox")?.innerText')
print(data)  # ~900 chars instead of ~97K
```

**Comparison for finding "Guido van Rossum" on Wikipedia:**

| Server | Method | Response Size |
|--------|--------|-------------:|
| Playwright MCP | `browser_snapshot` (dump full tree, search client-side) | ~495K chars |
| Chrome DevTools MCP | `take_snapshot` (dump full tree, search client-side) | ~538K chars |
| OpenBrowser MCP | `evaluate()` + Python string matching | ~900 chars |

## E2E LLM Benchmark

### Methodology

Six real-world browser tasks run through Claude Sonnet 4.6 on AWS Bedrock (Converse API) with a server-agnostic system prompt. Each task uses a single MCP server as tool provider. The LLM autonomously decides which tools to call and when the task is complete. All tasks run against live websites. Each server is benchmarked over 5 independent runs; statistics below report mean +/- standard deviation with 95% bootstrap confidence intervals (10,000 samples).

**Benchmark Tasks:**

| # | Task | What the LLM must do | Target Site | Verification |
|:-:|------|----------------------|-------------|--------------|
| 1 | **fact_lookup** | Navigate to the Python Wikipedia article and extract who created it and when | en.wikipedia.org | Must mention "Guido van Rossum" and "1991" |
| 2 | **form_fill** | Navigate to a form, fill in customer name, select pizza size and topping, submit | httpbin.org/forms/post | POST response must contain `custname=John Doe`, `size=medium`, `topping=mushroom` |
| 3 | **multi_page_extract** | Extract the titles of the top 5 stories from the Hacker News front page | news.ycombinator.com | Must return at least 3 non-empty story titles |
| 4 | **search_navigate** | Go to Wikipedia, search for "Rust programming language", click the result, identify the original developer | en.wikipedia.org | Must mention "Mozilla" |
| 5 | **deep_navigation** | Navigate to the claude-code GitHub repo and find the latest release version | github.com/anthropics/claude-code | Must contain a version pattern like "v1.2.3" |
| 6 | **content_analysis** | Visit example.com and describe the page structure: count headings, links, and paragraphs | example.com | Must mention "1" heading and "1" link |

The tasks span different browser automation capabilities: reading content, filling forms, navigating search results, extracting structured data, and analyzing page structure. The system prompt is server-agnostic -- it gives no hints about which tools are available or how to use them.

<img src="/images/benchmark_comparison.png" alt="E2E LLM Benchmark: Duration vs Bedrock API Token Usage" style={{width: "100%", maxWidth: 800}} />

### Results: Task Success (N=5 runs)

All three servers achieve **100% pass rate** (6/6 tasks) across all 5 runs.

### Results: Tool Calls and Duration (N=5 runs, mean +/- std)

| Metric | Playwright MCP | Chrome DevTools MCP | OpenBrowser MCP |
|--------|---------------:|--------------------:|----------------:|
| **Total tool calls** | 11.0 +/- 1.4 | 19.8 +/- 0.4 | 15.0 +/- 3.9 |
| **Total duration** | 92.2 +/- 11.4s | 128.8 +/- 6.2s | 103.1 +/- 16.4s |
| **95% CI (duration)** | 83.6 -- 101.0s | 123.9 -- 133.6s | 90.4 -- 115.8s |

Playwright completes most tasks in 1 tool call because every navigation returns the full accessibility snapshot -- the LLM sees the entire page immediately and can answer without follow-up queries. Chrome DevTools requires separate navigate + take_snapshot calls, resulting in more tool calls. OpenBrowser's CodeAgent batches navigation, extraction, and processing into single execute_code calls, achieving comparable speed with fewer total tokens.

<img src="/images/benchmark_tool_calls.png" alt="Tool Calls Per Task by MCP Server" style={{width: "100%", maxWidth: 800}} />

<img src="/images/benchmark_per_task_duration.png" alt="Per-Task Duration by MCP Server" style={{width: "100%", maxWidth: 800}} />

### Results: Bedrock API Token Usage

Total tokens billed by the Bedrock Converse API across 6 tasks (input + output). This is the actual LLM cost -- includes system prompt, tool schemas, conversation history, and tool results.

| Metric | Playwright MCP | Chrome DevTools MCP | OpenBrowser MCP |
|--------|---------------:|--------------------:|----------------:|
| **Input tokens** | 148,709 | 308,947 | 46,566 |
| **Output tokens** | 1,539 | 1,909 | 2,857 |
| **Total tokens** | **150,248** | **310,856** | **49,423** |
| **Ratio vs OpenBrowser** | **3.0x more** | **6.3x more** | **1x (baseline)** |

<img src="/images/benchmark_token_breakdown.png" alt="Total Bedrock API Token Usage" style={{width: "100%", maxWidth: 800}} />

Per-task Bedrock API input tokens:

| Task | Playwright MCP | Chrome DevTools MCP | OpenBrowser MCP |
|------|---------------:|--------------------:|----------------:|
| fact_lookup | 22,980 | 35,776 | 6,949 |
| form_fill | 20,030 | 53,507 | 10,556 |
| multi_page_extract | 24,133 | 34,623 | 4,113 |
| search_navigate | 56,031 | 155,027 | 13,386 |
| deep_navigation | 17,144 | 11,921 | 3,996 |
| content_analysis | 8,391 | 18,093 | 7,566 |

<img src="/images/benchmark_per_task_tokens.png" alt="Per-Task Input Token Usage by MCP Server" style={{width: "100%", maxWidth: 800}} />

### Results: Cost Per Benchmark Run (6 Tasks)

Based on Bedrock API token usage. Input tokens at input rate, output tokens at output rate.

| Model | Playwright MCP | Chrome DevTools MCP | OpenBrowser MCP |
|-------|---------------:|--------------------:|----------------:|
| Claude Sonnet 4.6 ($3/$15 per M) | $0.47 | $0.96 | **$0.18** |
| Claude Opus 4.6 ($5/$25 per M) | $0.78 | $1.59 | **$0.30** |

<img src="/images/benchmark_cost.png" alt="Cost Per Benchmark Run by Model" style={{width: "100%", maxWidth: 800}} />

### Results: MCP Response Size (Tool Output Only)

This measures only the text returned from MCP tool calls -- a subset of the total tokens. Useful for understanding architectural differences, but **not the same as total LLM cost**.

| Metric | Playwright MCP | Chrome DevTools MCP | OpenBrowser MCP |
|--------|---------------:|--------------------:|----------------:|
| **Total response chars** | 1,135,423 | 1,204,132 | 6,667 |
| **Est. response tokens** | 283,853 | 301,030 | 1,665 |
| **Response size ratio** | 170x more | 181x more | 1x (baseline) |

The large response size difference (170x) reflects the architectural difference: Playwright and Chrome DevTools dump full page accessibility snapshots as tool output, while OpenBrowser processes page state server-side and returns only extracted results. However, the actual LLM cost difference is **3x-6.3x** (not 170x), because total cost also includes system prompts, tool schemas, and conversation history that are similar across servers.

<img src="/images/benchmark_response_size.png" alt="MCP Response Size Comparison" style={{width: "100%", maxWidth: 800}} />

### Why OpenBrowser Uses More Tool Calls but Fewer Tokens

Playwright sends the full page with every response, so the LLM gets the answer immediately but pays for processing ~120K tokens per Wikipedia page load. OpenBrowser returns compact results (~30-800 chars per call). The LLM writes code that processes browser state server-side, so it never needs to see the full page content in its context window.

The tradeoff: OpenBrowser needs slightly more tool calls (code iterations) but each response is compact, resulting in a smaller total context window and lower API cost.

## Tool Surface Comparison

### Playwright MCP (22 core tools)

Navigation, interaction, form filling, file upload, drag, hover, key press, select option, screenshots, snapshots, console messages, dialog handling, network requests, tab management, code generation, PDF export, wait conditions.

### Chrome DevTools MCP (26 tools)

Input automation (click, drag, fill, fill_form, hover, press_key, handle_dialog, upload_file), navigation (navigate_page, new_page, close_page, list_pages, select_page, wait_for), emulation (emulate, resize_page), performance tracing (start/stop/analyze), network debugging (list/get requests), JS execution, console messages, screenshots, DOM snapshots.

### OpenBrowser MCP (1 tool -- CodeAgent)

| Tool | Capabilities |
|------|-------------|
| `execute_code` | Python code execution in a persistent namespace with browser automation functions: `navigate()`, `click()`, `input_text()`, `scroll()`, `go_back()`, `select_dropdown()`, `send_keys()`, `upload_file()`, `evaluate()` (JS), `switch()`/`close()` (tabs), `done()` (task completion). Also provides `file_system` for local file operations and pre-imported libraries (json, pandas, re, csv, etc.) |

## Unique to OpenBrowser

Features no competitor offers:

- **CodeAgent architecture** -- single `execute_code` tool runs Python in a persistent namespace. The LLM writes code to navigate, extract, and process data rather than calling individual tools. Variables and state persist between calls.
- **3-6x token efficiency** -- LLM writes code that processes browser state server-side, returning only extracted results instead of full page dumps
- **JS evaluation with Python processing** -- `await evaluate("JS expression")` returns Python objects directly (dicts, lists, strings), enabling pandas/regex/json processing in the same code block
- **Built-in libraries** -- json, pandas, numpy, matplotlib, csv, re, datetime, requests, BeautifulSoup available in the execution namespace
- **File system access** -- `file_system` object for reading/writing local files from browser automation code
- **Dropdown support** -- `select_dropdown()` and `dropdown_options()` for native `<select>` elements
- **Task completion signal** -- `done(text, success)` to explicitly mark task completion with a result

## Gaps vs Competitors

| Capability | Playwright MCP | Chrome DevTools MCP | OpenBrowser MCP |
|-----------|:-:|:-:|:-:|
| Cross-browser | Yes (Chromium/Firefox/WebKit) | No | No |
| Screenshots | Yes | Yes | No |
| File upload | Yes | Yes | Yes |
| PDF export | Yes | No | No |
| Network monitoring | Yes | Yes | No |
| Console logs | Yes | Yes | No |
| Performance tracing | No | Yes | No |
| Emulation (device/viewport) | No | Yes | No |
| Drag and drop | No | Yes | No |
| Dialog handling | No | Yes | No |
| Wait conditions | No | Yes | No |
| Code generation | Yes | No | No |
| Incremental snapshots | Yes | No | No |
| Accessibility tree | Via snapshot | Via snapshot | Via `evaluate()` JS queries |
| Content grep/search | No | No | Via `evaluate()` + Python |
| Progressive disclosure | No | No | Yes (code extracts only what's needed) |
| Session reuse | No | Puppeteer-managed | Yes (existing Chrome) |

## Benchmark Methodology Notes

### Token Metrics Explained

Two token metrics are reported:

1. **Bedrock API tokens** -- `inputTokens` + `outputTokens` from the Bedrock Converse API `usage` field. This is the actual LLM cost billed by AWS. Includes system prompt, tool schemas, conversation history (accumulated across turns), and tool results. This is the primary metric.

2. **MCP response size** -- total character count of text returned from MCP tool calls (estimated as chars / 4). This is a subset of the Bedrock input tokens. Useful for understanding architectural differences (how much data each server returns per tool call), but does not represent total LLM cost.

The response size ratio (170x) is much larger than the API token ratio (3-6.3x) because the total token count includes fixed costs (system prompt, tool schemas) and conversation history that are similar across servers.

### Token Usage Benchmark (5-step workflow)
- All three servers benchmarked via JSON-RPC stdio subprocess -- no estimates
- Response sizes measured as total JSON-RPC response character count
- Estimated tokens = characters / 4 (standard approximation for mixed English/JSON)
- 5-step workflow uses matched operations: navigate, get state/snapshot, click, go back, get state/snapshot
- Raw benchmark data: `benchmarks/playwright_results.json`, `benchmarks/cdp_results.json`, `benchmarks/openbrowser_results.json`

### E2E LLM Benchmark (6 real-world tasks)
- Model: Claude Sonnet 4.6 on AWS Bedrock (Converse API)
- Each task uses a single MCP server as tool provider via JSON-RPC stdio
- The LLM autonomously decides which tools to call and when the task is complete
- 6 tasks: fact_lookup, form_fill, multi_page_extract, search_navigate, deep_navigation, content_analysis
- All tasks run against live websites (Wikipedia, httpbin.org, news.ycombinator.com, github.com, example.com)
- Bedrock API tokens tracked via `response["usage"]["inputTokens"]` and `response["usage"]["outputTokens"]`
- MCP response sizes measured from actual tool response character counts
- Playwright MCP tested via `npx @playwright/mcp@latest`
- Chrome DevTools MCP tested via `npx -y chrome-devtools-mcp@latest`
- OpenBrowser MCP v0.1.22 tested via `uvx openbrowser-ai[mcp]==0.1.22 --mcp`
- Server-agnostic system prompt (no server-specific hints)
- 5 independent runs per server with bootstrap confidence intervals (10,000 samples)
- All tests run on macOS, M-series Apple Silicon
- Benchmark script: `benchmarks/e2e_llm_benchmark.py`, `benchmarks/e2e_llm_stats.py`
- Results: `benchmarks/e2e_llm_stats_results.json`
